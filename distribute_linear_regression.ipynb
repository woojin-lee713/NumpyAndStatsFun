{
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "hide": true,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# The %... is an iPython thing, and is not part of the Python language.\n",
        "# In this case we're just telling the plotting library to draw things on\n",
        "# the notebook, instead of on a separate window.\n",
        "%matplotlib inline\n",
        "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
        "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\newcommand{\\Ex}{\\mathbb{E}}\n",
        "\\newcommand{\\Var}{\\mathrm{Var}}\n",
        "\\newcommand{\\Cov}{\\mathrm{Cov}}\n",
        "\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n",
        "\\newcommand{\\indic}{\\mathbb{1}}\n",
        "\\newcommand{\\avg}{\\overline}\n",
        "\\newcommand{\\est}{\\hat}\n",
        "\\newcommand{\\trueval}[1]{#1^{*}}\n",
        "\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\renewcommand{\\like}{\\cal L}\n",
        "\\renewcommand{\\loglike}{\\ell}\n",
        "\\renewcommand{\\err}{\\cal E}\n",
        "\\renewcommand{\\dat}{\\cal D}\n",
        "\\renewcommand{\\hyp}{\\cal H}\n",
        "\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n",
        "\\renewcommand{\\x}{\\mathbf x}\n",
        "\\renewcommand{\\v}[1]{\\mathbf #1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Linear Regression MLE\n",
        "\n",
        "Linear regression is the workhorse algorithm thats used in many sciences, social and natural. The diagram below illustrates the probabilistic interpretation of linear regression, and the idea behind the MLE for linear regression. We illustrate a point $(x_i, y_i)$, and the corresponding prediction  for $x_i$ using the line, that is $yhat_i$ or $\\hat{y}_i$.\n",
        "\n",
        "![](images/linregmle.png)\n",
        "\n",
        "The fundamental assumption for the probabilistic analysis of linear regression is that each $y_i$ is gaussian distributed with mean  $\\v{w}\\cdot\\v{x_i}$ (the y predicted by the regression line so to speak) and variance $\\sigma^2$:\n",
        "\n",
        "$$ y_i \\sim N(\\v{w}\\cdot\\v{x_i}, \\sigma^2) .$$\n",
        "\n",
        "Here we have made $\\v{x_i}$ a vector, as well as $\\v{w}$, since, even in the one dimensional case w can write $\\v{x_i} = (1, x_i)$ and $\\v{w} = (a, b)$ since their dot product then gives us $a + b x_i$.\n",
        "\n",
        "We can then write the likelihood:\n",
        "\n",
        "$$\\cal{L} = p( \\{y\\} | \\{\\v{x}\\}, \\v{w}, \\sigma) = \\prod_i p(y_i | \\v{x}_i, \\v{w}, \\sigma)$$\n",
        "\n",
        "Given the canonical form of the gaussian:\n",
        "\n",
        "$$N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},$$\n",
        "\n",
        "we can show that:\n",
        "\n",
        "$$\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .$$\n",
        "\n",
        "The log likelihood $\\ell$ then is given by:\n",
        "\n",
        "$$\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .$$\n",
        "\n",
        "If you differentiate this with respect to  $\\v{w}$ and $\\sigma$, you get the MLE values of the parameter estimates:\n",
        "\n",
        "$$\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, $$\n",
        "\n",
        "where $\\v{X}$ is the design matrix created by stacking rows $\\v{x}_i$, and\n",
        "\n",
        "$$\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . $$\n",
        "\n",
        "These are the standard results of linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting a regression model in sklearn\n",
        "\n",
        "The dataset below consists of data generated from a straight line with a lot of noise, and a slope of 80.9 with intercept 0 and a noise standard deviation of 100.\n",
        "\n",
        "We're going to fit it with `sklearn` without going into the details of how it is fit. For linear regression this can be done analytically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/regression_data.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(df.x,df.y, 'o');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the purposes of drawing the regression line, lets create a uniform grid of points, and then reshape it into the canonical format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgrid = np.linspace(-2.5,2.5,1000)\n",
        "Xgrid = xgrid.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.x.values.reshape(-1,1) # reshape into list of lists format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape, df.y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X,df.y)\n",
        "fgrid = lr.predict(Xgrid)\n",
        "lr.coef_, lr.intercept_ # the noise gives us a different fit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(Xgrid, fgrid)\n",
        "plt.plot(X, df.y, '.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The impact of noise\n",
        "\n",
        "Lets see how this Gaussian noise affects us. We'll generate a new data set with the fit params from the first one, and add in the noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "fpredict = lr.predict(X)\n",
        "dataset = norm(fpredict, 100).rvs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(Xgrid, fgrid)\n",
        "plt.plot(X, df.y, 'o', alpha=0.2)\n",
        "plt.plot(X, dataset, 's', alpha=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The impact of sample size\n",
        "\n",
        "We'll sample 20 points from the data set. We do this by sampling 20 indices, index into X and y, and then fit on the sample. We'll do this again and again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_indices = np.random.choice(range(100), size=20)\n",
        "sample_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xsample = X[sample_indices]\n",
        "ysample = df.y.values[sample_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "slopes = np.zeros(100)\n",
        "intercepts = np.zeros(100)\n",
        "for i in range(100):\n",
        "    sample_indices = np.random.choice(range(100), size=20)\n",
        "    Xsample = X[sample_indices]\n",
        "    ysample = df.y[sample_indices]\n",
        "    lr = LinearRegression().fit(Xsample, ysample)\n",
        "    slopes[i] = lr.coef_\n",
        "    intercepts[i] = lr.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(slopes);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(intercepts);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpredicts = intercepts + slopes*Xgrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpredicts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(X, df.y, 'o', alpha=0.2)\n",
        "for line in range(100):\n",
        "    vals = fpredicts[:,line]\n",
        "    plt.plot(Xgrid, vals, color=\"r\", alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks narrow? Remember this is just the sampling distribution of the lines.\n",
        "\n",
        "Now let us add in one realization of a data set by adding in gaussian noise from each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(X, df.y, 'o', alpha=0.2)\n",
        "for line in range(100):\n",
        "    vals = fpredicts[:,line]\n",
        "    gaussvals = norm(vals, 100).rvs()\n",
        "    plt.plot(Xgrid, vals, color=\"r\", alpha=0.05)\n",
        "    plt.plot(Xgrid, gaussvals, '.', color=\"k\", alpha=0.009)"
      ]
    }
  ]
}