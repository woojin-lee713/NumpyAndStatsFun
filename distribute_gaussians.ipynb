{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hide": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Large Numbers\n",
    "\n",
    "Lets keep increasing the length of the sequence of coin flips n, and compute a running average $S_n$ of the coin-flip random variables,\n",
    "$$S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i .$$\n",
    "We plot this running mean, and notice that it converges to the mean of the distribution from which the random variables are plucked, ie the Bernoulli distribution with p=0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def throw_a_coin(n):\n",
    "    brv = bernoulli(0.5)\n",
    "    return brv.rvs(size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "random_flips = throw_a_coin(10000)\n",
    "running_means = np.zeros(10000)\n",
    "sequence_lengths = np.arange(1,10001,1)\n",
    "for i in sequence_lengths:\n",
    "    running_means[i-1] = np.mean(random_flips[:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "figure_caption": "The Law of Large Numbers: means of sequences converge to the distribution mean.",
    "figure_type": "m",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(sequence_lengths, running_means);\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is an example of a very important theorem in statistics, the law of large numbers, which says this:\n",
    "\n",
    "**Let $x_1,x_2,...,x_n$ be a sequence of independent, identically-distributed (IID) random variables. Suppose that $X$ has the finite mean $\\mu$. Then the average of the first n of them:**\n",
    "\n",
    "$$S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,$$\n",
    "\n",
    "**converges to the mean of the variables $\\mu$ as $n \\to \\infty$:**\n",
    "\n",
    "$$ S_n \\to \\mu \\, as \\, n \\to \\infty. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Statistics\n",
    "\n",
    "Answers the question: **What is Data?** with\n",
    "\n",
    ">\"data is a **sample** from an existing **population**\"\n",
    "\n",
    "- data is stochastic, variable, in the sense that you can draw different samples\n",
    "- model the sample. The model may have parameters\n",
    "- The parameters are considered **FIXED**, and there is a **true value** in our population\n",
    "- However, we can only find parameters for our sample, since in real-life we usually only get to see one sample.\n",
    "- If we could somehow access multiple samples, these parameters would vary from sample to sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Samples from a population of coin flips\n",
    "\n",
    "Having now established something about long sequences of random variables, lets turn to samples from the population of random numbers.\n",
    "\n",
    "Lets redo the experiment with coin flips that we started earlier.\n",
    "\n",
    "We'll establish some terminology at first. What we'll do different here is to do a large set of **replications** M, in each of which we did many coin flips, or **observations** N.  We'll call **a single replication a sample of observations**. Thus the number of samples is M, and the sample size is N. \n",
    "\n",
    "![](images/grid.png)\n",
    "\n",
    "These samples have been chosen from a population of size $n >> N$. If this reminds you of elections, thats intentional.\n",
    "\n",
    "We'll now calculatethe mean over the observations in a sample, or sample mean, for a sample size of 10, with 20 replications. There are thus 20 means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_throws(number_of_samples, sample_size):\n",
    "    start=np.zeros((number_of_samples, sample_size), dtype=int)\n",
    "    for i in range(number_of_samples):\n",
    "        start[i,:] = throw_a_coin(sample_size)\n",
    "    return np.mean(start, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "make_throws(number_of_samples=20, sample_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives us 20 means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing sample size, and number of replications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now do 200 replications, each of which has a sample size of 1000 flips, and store the 200 means for each sample size from 1 to 1000 (in strides of 10) in `sample_means`. This will take a bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_sizes=np.arange(1,1001,10)\n",
    "sample_means = [make_throws(number_of_samples=200, sample_size=i) for i in sample_sizes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets formalize what we are up to. Lets call the N random variables in the $m^{th}$ sample $x_{m1},x_{m2},...,x_{mN}$ and lets define the sample mean\n",
    "\n",
    "$$\\bar{x_m}(N) = \\frac{1}{N}\\, \\sum_{i=1}^{N} x_{mi} $$\n",
    "\n",
    "Now imagine the size of the sample becoming large, asymptoting to the size of an infinite or very large population (ie the sample becomes the population). Then you would expect the sample mean to approach the mean of the population distribution. This is just a restatement of the law of large numbers.\n",
    "\n",
    "Of course, if you drew many different samples of a size N (which is not infinite), the sample means $\\bar{x_1}$, $\\bar{x_2}$, etc would all be a bit different from each other. But the law of large numbers intuitively indicates that as the sample size gets very large and becomes an infinite population size, these slightly differeing means would all come together and converge to the population (or distribution) mean.\n",
    "\n",
    "To see this lets define, instead, the mean or expectation of the sample means over the set of samples or replications, at a sample size N:\n",
    "\n",
    "$$E_{\\{R\\}}(\\bar{x}) = \\frac{1}{M} \\,\\sum_{m=1}^{M} \\bar{x_m}(N) ,$$\n",
    "where $\\{R\\}$ is the set of M replications, and calculate and plot this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mean_of_sample_means = [np.mean(means) for means in sample_means]\n",
    "len(mean_of_sample_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "figure_caption": "The mean of sample means also approaches the distribution mean.",
    "figure_type": "m",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, mean_of_sample_means);\n",
    "plt.ylim([0.480,0.520]);\n",
    "plt.axhline(0.5, 0, 1, color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the mean of the sample means converges to the distribution mean as the sample size N gets very large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notion of a Sampling Distribution\n",
    "\n",
    "In data science, we are always interested in understanding the world from incomplete data, in other words from a sample or a few samples of a population at large. Our experience with the world tells us that even if we are able to repeat an experiment or process, we will get more or less different answers the next time. If all of the answers were very different each time, we would never be able to make any predictions.\n",
    "\n",
    "But some kind of answers differ only a little, especially as we get to larger sample sizes. So the important question then becomes one of the distribution of these quantities from sample to sample, also known as a **sampling distribution**. \n",
    "\n",
    "Since, in the real world, we see only one sample, this distribution helps us do **inference**, or figure the uncertainty of the estimates of quantities we are interested in. If we can somehow cook up samples just somewhat different from the one we were given (this is the idea behind the bootstrap), we can calculate quantities of interest, such as the mean on each one of these samples. By seeing how these means vary from one sample to the other, we can say how typical the mean in the sample we were given is, and whats the uncertainty range of this quantity. This is why the mean of the sample means is an interesting quantity; it characterizes the **sampling distribution of the mean**, or the distribution of sample means.\n",
    "\n",
    "We can see this mathematically by writing the mean or expectation value of the sample means thus:\n",
    "\n",
    "$$E_{\\{R\\}}(N\\,\\bar{x}) = E_{\\{R\\}}(x_1 + x_2 + ... + x_N) = E_{\\{R\\}}(x_1) + E_{\\{R\\}}(x_2) + ... + E_{\\{R\\}}(x_N)$$\n",
    "\n",
    "Now in the limit of a very large number of replications, each of the expectations in the right hand side can be replaced by the population mean using the law of large numbers! Thus:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\mu\\\\\n",
    "E(\\bar{x}) &=& \\mu\n",
    "\\end{eqnarray*}\n",
    "\n",
    "which tells us that in the limit of a large number of replications the expectation value of the sampling means converges to the population mean. This limit gives us the true sampling distribution, as opposed to what we might estimate from our finite set of replicates.\n",
    "\n",
    "Lets make many replicates (M=1000) instead to see what the \"true\" sampling distribution looks like as a function of sample size.(Caution: This takes a few minutes to run!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "figure_caption": "With more replicates, this process is much faster!",
    "figure_type": "m",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_means_1000_replicates = [make_throws(number_of_samples=1000, sample_size=i) for i in sample_sizes]\n",
    "mean_of_sample_means_1000 = [np.mean(means) for means in sample_means_1000_replicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, mean_of_sample_means_1000);\n",
    "plt.ylim([0.480,0.520]);\n",
    "plt.axhline(0.5, 0, 1, color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sampling distribution as a function of sample size\n",
    "\n",
    "We can see what the estimated sampling distribution of the mean looks like at different sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the distribution of the mean as a function of sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes[1], sample_sizes[40], sample_sizes[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200 replicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 40, 99]:\n",
    "    plt.scatter([sample_sizes[i]]*200, sample_means[i], alpha=0.03);\n",
    "plt.xlim([0,1000])\n",
    "plt.ylim([0.25,0.75]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 replicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "figure_type": "m",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in [1, 40, 99]:\n",
    "    plt.scatter([sample_sizes[i]]*1000, sample_means_1000_replicates[i], alpha=0.03);\n",
    "plt.xlim([0,1000])\n",
    "plt.ylim([0.25,0.75]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "std_of_sample_means_1000 = [np.std(means) for means in sample_means_1000_replicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "figure_caption": "The slope of the graph is -0.5 showing the inverse proportion to the square root of N",
    "figure_type": "m",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(sample_sizes), np.log10(std_of_sample_means_1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.diff(np.log10(std_of_sample_means_1000))/np.diff(np.log10(sample_sizes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the power of negative half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot again the distribution of sample means at a large sample size, $N=1000$. What distribution is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(sample_means[99], bins=np.arange(0.4,0.6,0.005));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **Gaussian** with sample standard deviation scaling as $1/\\sqrt{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gaussian Distribution\n",
    "\n",
    "The sampling distribution of the mean itself has a mean $\\mu$ and variance $s^2 = \\frac{\\sigma^2}{N}$. This distribution is called the **Gaussian** or **Normal Distribution**, and is probably the most important distribution in all of statistics.\n",
    "\n",
    "The probability density of the normal distribution is given as:\n",
    "\n",
    "$$ N(x, \\mu, \\sigma) = \\frac{1}{s\\sqrt{2\\pi}} e^{ -\\frac{(x-\\mu)^2}{2s^2} } .$$\n",
    "\n",
    "$s$ is called the **standard error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets step back and try and think about what this all means. As an example, say I have a weight-watchers' study of 1000 people, whose average weight is 150 lbs with standard deviation of 30lbs. If I was to randomly choose many samples of 100 people each, the mean weights of those samples would cluster around 150lbs with a standard error of 30/$\\sqrt{100}$ = 3lbs. Now if i gave you a different sample of 100 people with an average weight of 170lbs, this weight would be more than 6 standard errors beyond the population mean, ^[this example is motivated by the crazy bus example in Charles Whelan's excellent Naked Statistics Book] and would thus be very unlikely to be from the weight watchers group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected value of the Gaussian distribution is $E[X]=\\mu$ and the variance is $Var[X]=s^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "\n",
    "The reason for the distribution's importance is the Central Limit Theorem(CLT). The theorem is stated as thus, very similar to the law of large numbers:\n",
    "\n",
    "**Let $x_1,x_2,...,x_n$ be a sequence of independent, identically-distributed (IID) random variables from a random variable $X$. Suppose that $X$ has the finite mean $\\mu$ AND finite variance $\\sigma^2$. Then the average of the first n of them:**\n",
    "\n",
    "$$S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,$$\n",
    "\n",
    "**converges to a Gaussian Random Variable with mean $\\mu$ and variance $\\sigma^2/n$ as $n \\to \\infty$:**\n",
    "\n",
    "$$ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. $$\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$s^2 = \\frac{\\sigma^2}{N}.$$\n",
    "\n",
    "\n",
    "This is true, *regardless* of the shape of $X$, which could be binomial, poisson, or any other distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, under some conditions ^[REF :Lyapunov conditions], the variables $x_i$ dont have to be identically distributed, as long as $\\mu$ is the mean of the means and $\\sigma^2$ is the sum of the individual variances. This has major consequences, for the importance of this theorem.\n",
    "\n",
    "Many random variables can be thought of as having come from the sum of a large number of small and independent effects. For example human height or weight can be thought of as the sum as a large number of genetic and environmental factors, which add to increase or decrease height or weight respectively. Or think of a measurement of a height. There are lots of ways things could go wrong: frayed tapes, stretched tapes, smudged marks, bad lining up of the eye, etc. These are all independent and have no systematic error in one direction or the other.\n",
    "\n",
    "Then the sum of these factors, as long as there are a large number of them, will be distributed as a gaussian.\n",
    "\n",
    "As a rule of thumb, the CLT starts holding at $N \\sim 30$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo as a function of number of samples\n",
    "\n",
    "How does the accuracy depends on the number of points(samples)? Lets try the same 1-D integral $ I= \\int_{2}^{3} [x^2 + 4 \\, x \\,\\sin(x)] \\, dx $ as a function of the number of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 4*x*np.sin(x) \n",
    "\n",
    "def intf(x): \n",
    "    return x**3/3.0+4.0*np.sin(x) - 4.0*x*np.cos(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2;    \n",
    "b = 3;\n",
    "Imc=np.zeros(1000)\n",
    "Na = np.linspace(0,1000,1000)\n",
    "\n",
    "exactval= intf(b)-intf(a)\n",
    "\n",
    "for N in np.arange(0,1000):\n",
    "    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n",
    "    Y =f(X)   # CALCULATE THE f(x) \n",
    "\n",
    "    Imc[N]= (b-a) * np.sum(Y)/ N;\n",
    "    \n",
    "    \n",
    "plt.plot(Na[10:],np.sqrt((Imc[10:]-exactval)**2), alpha=0.7)\n",
    "plt.plot(Na[10:], 1/np.sqrt(Na[10:]), 'r')\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"sqrt((Imc-ExactValue)$^2$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this depends on the number of $N$ as $1/\\sqrt{N}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors in MC\n",
    "\n",
    "Monte Carlo methods yield approximate\n",
    "answers whose accuracy depends on the number of draws.\n",
    "So far, we have used our knowledge of the exact value  to determine that the\n",
    "error in the Monte Carlo method approaches zero as approximately $1/\\sqrt{N}$ for large $N$, where $N$ is the number of trials. \n",
    "\n",
    "But in the usual case, the exact answer is unknown. Why do this otherwise?\n",
    "\n",
    "So, lets repeat the same evaluation $m$ times and check the variance of the estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple MC estimations\n",
    "m=1000\n",
    "N=10000\n",
    "Imc=np.zeros(m)\n",
    "\n",
    "\n",
    "for i in np.arange(m):\n",
    "    \n",
    "    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n",
    "    Y =f(X)   # CALCULATE THE f(x) \n",
    "\n",
    "    Imc[i]= (b-a) * np.sum(Y)/ N;\n",
    "    \n",
    "    \n",
    "plt.hist(Imc, bins=30)\n",
    "plt.xlabel(\"Imc\")\n",
    "print(np.mean(Imc), np.std(Imc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like our telltale Normal distribution.\n",
    "\n",
    "This is not surprising\n",
    "\n",
    "### Estimating the error in MC integration using the CLT.\n",
    "\n",
    "We know from the CLT that if $x_1,x_2,...,x_n$ be a sequence of independent, identically-distributed (IID) random variables from a random variable $X$, and that if $X$ has the finite mean $\\mu$ AND finite variance $\\sigma^2$. \n",
    "\n",
    "Then, \n",
    "\n",
    "$$S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,$$\n",
    "\n",
    "**converges to a Gaussian Random Variable with mean $\\mu$ and variance $\\sigma^2/n$ as $n \\to \\infty$:**\n",
    "\n",
    "$$ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. $$\n",
    "\n",
    "\n",
    "This is true *regardless* of the shape of $X$, which could be binomial, poisson, or any other distribution.\n",
    "\n",
    "The sums\n",
    "\n",
    "$$S_n(f) = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) $$\n",
    "\n",
    "are exactly what we want to calculate for Monte-Carlo Integration(due to the LOTUS) and correspond to the random variable f(X) where X is uniformly distributed on the support.\n",
    "\n",
    "Whatever the original variance of f(X) might be, we can see that the variance of the sampling distribution of the mean goes down as $1/n$ and thus the standard error goes down as $1/\\sqrt{n}$ as we discovered when we compared it to the exact value as well.\n",
    "\n",
    "Why is this important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Comparing to standard integration techniques\n",
    "\n",
    "What if we changed the dimensionality of the integral? The formula for $S_n$ does not change, we just replace $g(x_i)$ by $g(x_i, y_i, z_i...)$. Thus, the CLT still holds and the error still scales as $\\frac{1}{\\sqrt{n}}$.\n",
    "\n",
    "On the other hand, if we divide the $a, b$-interval into $N$ \n",
    "steps and use some regular integration routine, what is the error? Consider the midpoint rule as illustrated in this diagram from Wikipedia:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/5b/Rectangle_rule.gif)\n",
    "\n",
    "The basic idea is that the function value at the midpoint of the interval is used as the height of the approximating rectangle. In general, the differing methods consist of choosing different $x_i$ below..with left being at the left end, right being at the right end. \n",
    "$$I(est) = \\sum_i f(x_i)\\Delta x_i = \\frac{b-a}{n} \\sum_i f(x_i)$$\n",
    "\n",
    "The error on the estimation of the integral can be shown to decrease as $\\frac{1}{n^2}$. The basic reason for this can be understood on a taylor series expansion of the function to second order. When you integrate on the sub-interval, the linear term vanishes while the quadratic term becomes cubic in $\\Delta x$. So the local error goes as $\\frac{1}{n^3}$ and thus the global as  $\\frac{1}{n^2}$.\n",
    "\n",
    "Monte-Carlo if clearly not competitive with the midpoint method in 1-D. Its actually not even competitive with left or right rectangle methods.\n",
    "\n",
    "The trapeziod rule uses a line between the sub-interval points while the  Simpsons rule uses a quadratic.\n",
    "\n",
    "These integrations can be generalized  to multiple dimensions, and the rule for these\n",
    "\n",
    "\n",
    "* left or right rule:  $\\propto 1/n$\n",
    "* Midpoint rule: $\\propto 1/n^2$\n",
    "* Trapezoid: $\\propto 1/n^2$\n",
    "* Simpson: $\\propto 1/n^4$\n",
    "\n",
    "where $n=N^{1/d}$. MC becomes better than the Simpson method only in 8 dimensions.."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
